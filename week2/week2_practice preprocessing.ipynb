{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python and str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[check by yourself](https://www.programiz.com/python-programming/methods/string)\n",
    "- slicing \n",
    "- listing\n",
    "- \\[::-1\\]\n",
    "- join\n",
    "- split\n",
    "- lower\n",
    "- upper\n",
    "- count\n",
    "- startswith\n",
    "- edswith\n",
    "- formating \n",
    "- strip \n",
    "- encoding \n",
    "- u, r, b, f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Мама, Мыла, Раму'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мама, HHH Мыла fff Раму'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Мама, {y} Мыла {x} Раму'.format(y='HHH', x='fff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мама, Мыла HHH Раму RR'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Мама, Мыла {} Раму {}'.format('HHH', 'RR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мама, Мыла gggg Раму rrrrr'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Мама, Мыла %s Раму %s' % ('gggg', 'rrrrr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мама, Мыла LLL Раму'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'LLL'\n",
    "f'Мама, Мыла {x} Раму'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мама, Мыла \n",
      " Раму\n"
     ]
    }
   ],
   "source": [
    "print('Мама, Мыла \\n Раму')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.endswith('Раму')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.index('Р')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мама, Мыла, Раму'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.encode('utf-8').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мама\n",
      " Мыла\n",
      " Раму\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(a.split(',')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng\n",
    "en_text = \"I want to study NLP's techniques, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ru\n",
    "ru_text = \"\"\"\n",
    "Умрёшь — начнешь опять сначала\n",
    "И повторится всё, как встарь:\n",
    "Ночь, ледяная рябь канала,\n",
    "Аптека, улица, фонарь.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(t, locale='en'):\n",
    "    from nltk.corpus import stopwords\n",
    "    from autocorrect import Speller\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    text = t.lower()\n",
    "    \n",
    "    if locale == 'ru':\n",
    "        stop_words = set(stopwords.words('russian'))\n",
    "        spell = Speller(lang='ru')\n",
    "    elif locale == 'en':\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        spell = Speller(lang='en')\n",
    "    else:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        spell = Speller(lang='en')\n",
    "        \n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "    filtered_sentence = [spell(w) for w in word_tokens if not spell(w) in stop_words and spell(w).strip() not in punctuation]\n",
    "\n",
    "    print(t)\n",
    "    print('to')\n",
    "    print(word_tokens)\n",
    "    print('to')\n",
    "    print(filtered_sentence)\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'study', \"NLP's\", 'techniques,', \"isn't\", 'it?']\n"
     ]
    }
   ],
   "source": [
    "print(en_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'study', \"NLP's\", 'techniques,', \"isn't\", 'it?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "vec = tokenizer.tokenize(en_text)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'study', 'NLP', \"'s\", 'techniques', ',', 'is', \"n't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "|  The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n",
    "|  This is the method that is invoked by ``word_tokenize()``.  It assumes that the\n",
    "|  text has already been segmented into sentences, e.g. using ``sent_tokenize()``.\n",
    "|  \n",
    "|  This tokenizer performs the following steps:\n",
    "|  \n",
    "|  - split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n",
    "|  - treat most punctuation characters as separate tokens\n",
    "|  - split off commas and single quotes, when followed by whitespace\n",
    "|  - separate periods that appear at the end of line'''\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "vec2 = tokenizer.tokenize(en_text)\n",
    "print(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['treewidth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('treewidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'study', 'NLP', \"'s\", 'techniques', ',', 'is', \"n't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(en_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'study', 'NLP', \"'\", 's', 'techniques', ',', 'isn', \"'\", 't', 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "|  Tokenize a text into a sequence of alphabetic and\n",
    "|  non-alphabetic characters, using the regexp ``\\w+|[^\\w\\s]+``.'''\n",
    "\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "vec3 = tokenizer.tokenize(en_text)\n",
    "print(vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 12, 14)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec), len(vec2), len(vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'study', \"NLP's\", 'techniques,', \"isn't\", 'it?']\n",
      "['I', 'want', 'to', 'study', 'NLP', \"'s\", 'techniques', ',', 'is', \"n't\", 'it', '?']\n",
      "['I', 'want', 'to', 'study', 'NLP', \"'\", 's', 'techniques', ',', 'isn', \"'\", 't', 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "print(vec)\n",
    "print(vec2)\n",
    "print(vec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nУмрёшь', '—', 'начнешь', 'опять', 'сначала\\nИ', 'повторится', 'всё,', 'как', 'встарь:\\nНочь,', 'ледяная', 'рябь', 'канала,\\nАптека,', 'улица,', 'фонарь.']\n"
     ]
    }
   ],
   "source": [
    "print(ru_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Умрёшь', '—', 'начнешь', 'опять', 'сначала', 'И', 'повторится', 'всё,', 'как', 'встарь:', 'Ночь,', 'ледяная', 'рябь', 'канала,', 'Аптека,', 'улица,', 'фонарь.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "ru_vec = tokenizer.tokenize(ru_text)\n",
    "print(ru_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Умрёшь', '—', 'начнешь', 'опять', 'сначала', 'И', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'Ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'Аптека', ',', 'улица', ',', 'фонарь', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "ru_vec2 = tokenizer.tokenize(ru_text)\n",
    "print(ru_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Умрёшь', '—', 'начнешь', 'опять', 'сначала', 'И', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'Ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'Аптека', ',', 'улица', ',', 'фонарь', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(ru_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Умрёшь', '—', 'начнешь', 'опять', 'сначала', 'И', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'Ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'Аптека', ',', 'улица', ',', 'фонарь', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "ru_vec3 = tokenizer.tokenize(ru_text)\n",
    "print(ru_vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 24, 24)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ru_vec), len(ru_vec2), len(ru_vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Умрёшь', '—', 'начнешь', 'опять', 'сначала', 'И', 'повторится', 'всё,', 'как', 'встарь:', 'Ночь,', 'ледяная', 'рябь', 'канала,', 'Аптека,', 'улица,', 'фонарь.'] \n",
      "\n",
      "['Умрёшь', '—', 'начнешь', 'опять', 'сначала', 'И', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'Ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'Аптека', ',', 'улица', ',', 'фонарь', '.'] \n",
      "\n",
      "['Умрёшь', '—', 'начнешь', 'опять', 'сначала', 'И', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'Ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'Аптека', ',', 'улица', ',', 'фонарь', '.']\n"
     ]
    }
   ],
   "source": [
    "print(ru_vec,'\\n')\n",
    "print(ru_vec2, '\\n')\n",
    "print(ru_vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'study', 'NLP', \"'s\", 'techniques', ',', 'is', \"n't\", 'it', '?']\n",
      "['I', 'want', 'study', 'NLP', \"'s\", 'techniques', ',', \"n't\", '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(en_text)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Умрёшь', '—', 'начнешь', 'опять', 'сначала', 'И', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'Ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'Аптека', ',', 'улица', ',', 'фонарь', '.'] \n",
      "\n",
      "['Умрёшь', '—', 'начнешь', 'сначала', 'повторится', 'всё', ',', 'встарь', ':', 'Ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'Аптека', ',', 'улица', ',', 'фонарь', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "word_tokens = word_tokenize(ru_text)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "print(word_tokens, '\\n')\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['будто', 'тогда', 'всю', 'чтобы', 'нас', 'мы', 'себя', 'раз', 'эти', 'какая']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stop_words)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation += '—'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~—\n"
     ]
    }
   ],
   "source": [
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = [w for w in word_tokens if not w.lower() in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Умрёшь',\n",
       " 'начнешь',\n",
       " 'опять',\n",
       " 'сначала',\n",
       " 'И',\n",
       " 'повторится',\n",
       " 'всё',\n",
       " 'как',\n",
       " 'встарь',\n",
       " 'Ночь',\n",
       " 'ледяная',\n",
       " 'рябь',\n",
       " 'канала',\n",
       " 'Аптека',\n",
       " 'улица',\n",
       " 'фонарь']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Speller in module autocorrect:\n",
      "\n",
      "class Speller(builtins.object)\n",
      " |  Speller(lang='en', threshold=0, nlp_data=None, fast=False, only_replacements=False)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__ = autocorrect_sentence(self, sentence)\n",
      " |  \n",
      " |  __init__(self, lang='en', threshold=0, nlp_data=None, fast=False, only_replacements=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  autocorrect_sentence(self, sentence)\n",
      " |  \n",
      " |  autocorrect_word(self, word)\n",
      " |      most likely correction for everything up to a double typo\n",
      " |  \n",
      " |  existing(self, words)\n",
      " |      {'the', 'teh'} => {'the'}\n",
      " |  \n",
      " |  get_candidates(self, word)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Speller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "message\n",
      "service\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(spell('caaar'))\n",
    "print(spell(u'mussage'))\n",
    "print(spell(u'survice'))\n",
    "print(spell(u'hte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\user\\anaconda3\\lib\\site-packages (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary for this language not found, downloading...\n",
      "__________________________________________________\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "done!\n",
      "машина\n",
      "общение\n",
      "практика\n",
      "координация\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='ru')\n",
    "print(spell(r'мышина'))\n",
    "print(spell(r'сбщение'))\n",
    "print(spell(r'проктика'))\n",
    "print(spell(r'каординация'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6558, 'общение'), (16, 'собщение')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.get_candidates('сбщение')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (USA) or America, is a federal republic composed of 50 states\n",
      "the united states of america (usa) or america, is a federal republic composed of 50 states\n"
     ]
    }
   ],
   "source": [
    "text = \"The United States of America (USA) or America, is a federal republic composed of 50 states\"\n",
    "print(text)\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to study NLP's techniques, isn't it?\n",
      "to\n",
      "['i', 'want', 'to', 'study', 'nlp', \"'s\", 'techniques', ',', 'is', \"n't\", 'it', '?']\n",
      "to\n",
      "['want', 'study', 'nl', \"'s\", 'techniques', \"n't\"]\n",
      "to\n",
      "['want', 'studi', 'nl', \"'s\", 'techniqu', \"n't\"]\n"
     ]
    }
   ],
   "source": [
    "en_stemming_sentence = [ps.stem(w) for w in custom_tokenizer(en_text)]\n",
    "print('to')\n",
    "print(en_stemming_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('talked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Умрёшь — начнешь опять сначала\n",
      "И повторится всё, как встарь:\n",
      "Ночь, ледяная рябь канала,\n",
      "Аптека, улица, фонарь.\n",
      "to\n",
      "['умрёшь', '—', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'аптека', ',', 'улица', ',', 'фонарь', '.']\n",
      "to\n",
      "['умрёшь', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', 'как', 'встарь', 'ночь', 'ледяная', 'рябь', 'канала', 'аптека', 'улица', 'фонарь']\n",
      "to\n",
      "['умрёшь', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', 'как', 'встарь', 'ночь', 'ледяная', 'рябь', 'канала', 'аптека', 'улица', 'фонарь']\n"
     ]
    }
   ],
   "source": [
    "ru_stemming_sentence = [ps.stem(w) for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_stemming_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "rs = RussianStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Умрёшь — начнешь опять сначала\n",
      "И повторится всё, как встарь:\n",
      "Ночь, ледяная рябь канала,\n",
      "Аптека, улица, фонарь.\n",
      "to\n",
      "['умрёшь', '—', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'аптека', ',', 'улица', ',', 'фонарь', '.']\n",
      "to\n",
      "['умрёшь', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', 'как', 'встарь', 'ночь', 'ледяная', 'рябь', 'канала', 'аптека', 'улица', 'фонарь']\n",
      "to\n",
      "['умреш', 'начнеш', 'опя', 'снача', 'и', 'повтор', 'все', 'как', 'встар', 'ноч', 'ледян', 'ряб', 'кана', 'аптек', 'улиц', 'фонар']\n"
     ]
    }
   ],
   "source": [
    "ru_stemming_sentence = [rs.stem(w) for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_stemming_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to study NLP's techniques, isn't it?\n",
      "to\n",
      "['i', 'want', 'to', 'study', 'nlp', \"'s\", 'techniques', ',', 'is', \"n't\", 'it', '?']\n",
      "to\n",
      "['want', 'study', 'nl', \"'s\", 'techniques', \"n't\"]\n",
      "to\n",
      "['want', 'study', 'nl', \"'s\", 'technique', \"n't\"]\n"
     ]
    }
   ],
   "source": [
    "en_lemm_sentence = [lemmatizer.lemmatize(w) for w in custom_tokenizer(en_text)]\n",
    "print('to')\n",
    "print(en_lemm_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Умрёшь — начнешь опять сначала\n",
      "И повторится всё, как встарь:\n",
      "Ночь, ледяная рябь канала,\n",
      "Аптека, улица, фонарь.\n",
      "to\n",
      "['умрёшь', '—', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'аптека', ',', 'улица', ',', 'фонарь', '.']\n",
      "to\n",
      "['умрёшь', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', 'как', 'встарь', 'ночь', 'ледяная', 'рябь', 'канала', 'аптека', 'улица', 'фонарь']\n",
      "to\n",
      "['умрёшь', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', 'как', 'встарь', 'ночь', 'ледяная', 'рябь', 'канала', 'аптека', 'улица', 'фонарь']\n"
     ]
    }
   ],
   "source": [
    "ru_lemm_sentence = [lemmatizer.lemmatize(w) for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_lemm_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from pymystem3) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pymystem3) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->pymystem3) (3.0.4)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 1\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem() # lemmatize, analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pymystem](https://github.com/nlpub/pymystem3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['стоять', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(m.lemmatize('Стоять'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Умрёшь — начнешь опять сначала\n",
      "И повторится всё, как встарь:\n",
      "Ночь, ледяная рябь канала,\n",
      "Аптека, улица, фонарь.\n",
      "to\n",
      "['умрёшь', '—', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'аптека', ',', 'улица', ',', 'фонарь', '.']\n",
      "to\n",
      "['умрёшь', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', 'как', 'встарь', 'ночь', 'ледяная', 'рябь', 'канала', 'аптека', 'улица', 'фонарь']\n",
      "to\n",
      "['умирать', 'начинать', 'опять', 'сначала', 'и', 'повторяться', 'все', 'как', 'встарь', 'ночь', 'ледяной', 'рябь', 'канал', 'аптека', 'улица', 'фонарь']\n"
     ]
    }
   ],
   "source": [
    "ru_lemm_sentence = [m.lemmatize(w)[0] for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_lemm_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var 2\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pymorphy](https://pymorphy2.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Умрёшь — начнешь опять сначала\n",
      "И повторится всё, как встарь:\n",
      "Ночь, ледяная рябь канала,\n",
      "Аптека, улица, фонарь.\n",
      "to\n",
      "['умрёшь', '—', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', ',', 'как', 'встарь', ':', 'ночь', ',', 'ледяная', 'рябь', 'канала', ',', 'аптека', ',', 'улица', ',', 'фонарь', '.']\n",
      "to\n",
      "['умрёшь', 'начнешь', 'опять', 'сначала', 'и', 'повторится', 'всё', 'как', 'встарь', 'ночь', 'ледяная', 'рябь', 'канала', 'аптека', 'улица', 'фонарь']\n",
      "to\n",
      "['умереть', 'начать', 'опять', 'сначала', 'и', 'повториться', 'всё', 'как', 'встарь', 'ночь', 'ледяной', 'рябь', 'канал', 'аптека', 'улица', 'фонарь']\n"
     ]
    }
   ],
   "source": [
    "ru_lemm_sentence2 = [morph.parse(w)[0].normal_form for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_lemm_sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipline:\n",
    "- find unique words\n",
    "- enumerate it, map with dictionary\n",
    "- create zero vectors \n",
    "- fill the values inside loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'There used to be Stone Age\\nThere used to be Bronze Age\\nThere used to be Iron Age\\nThere was Age of Revolution\\nNow it is Digital Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There used to be Stone Age',\n",
       " 'There used to be Bronze Age',\n",
       " 'There used to be Iron Age',\n",
       " 'There was Age of Revolution',\n",
       " 'Now it is Digital Age']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.split('\\n') # \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(text):\n",
    "    wordfreq = {}\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "    return wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'There': 4,\n",
       " 'used': 3,\n",
       " 'to': 3,\n",
       " 'be': 3,\n",
       " 'Stone': 1,\n",
       " 'Age': 5,\n",
       " 'Bronze': 1,\n",
       " 'Iron': 1,\n",
       " 'was': 1,\n",
       " 'of': 1,\n",
       " 'Revolution': 1,\n",
       " 'Now': 1,\n",
       " 'it': 1,\n",
       " 'is': 1,\n",
       " 'Digital': 1}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_bow = bow(t)\n",
    "t_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF\n",
    "\n",
    "TF термина а = (Количество раз, когда термин а встретился в тексте / количество всех слов в тексте)\n",
    "\n",
    "IDF термина а = логарифм(Общее количество документов / Количество документов, в которых встречается термин а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "   'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    " ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
       "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
       "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.384085  \n",
       "1  0.000000  0.281089  \n",
       "2  0.511849  0.267104  \n",
       "3  0.000000  0.384085  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.todense(), columns= vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ДЗ: проделать все то же самое с gensim и spaCy (lemmatization)\n",
    "\n",
    "- [Инструкция gensim](https://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors)\n",
    "- [Инструкция spaCy](https://spacy.io/usage/linguistic-features#tokenization)\n",
    "- [Инструкция spaCy 2](https://habr.com/ru/post/504680/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
