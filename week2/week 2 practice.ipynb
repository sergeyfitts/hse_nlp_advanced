{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python and str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[check by yourself](https://www.programiz.com/python-programming/methods/string)\n",
    "- slicing \n",
    "- listing\n",
    "- \\[::-1\\]\n",
    "- join\n",
    "- split\n",
    "- lower\n",
    "- upper\n",
    "- count\n",
    "- startswith\n",
    "- edswith\n",
    "- formating \n",
    "- strip \n",
    "- encoding \n",
    "- u, r, b, f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Мама, Мыла, Раму'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Мама, Мыла {x} Раму'.format(x='HHH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Мама, Мыла %s Раму' % ('gggg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'LLL'\n",
    "f'Мама, Мыла {x} Раму'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(r'Мама, Мыла \\n Раму')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.endswith('Раму')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.index('Р')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.encode('utf-8').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(a.split(',')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng\n",
    "en_text = \"I want to study NLP's techniques, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ru\n",
    "ru_text = \"\"\"\n",
    "Умрёшь — начнёшь опять сначала\n",
    "И повторится всё, как встарь:\n",
    "Ночь, ледяная рябь канала,\n",
    "Аптека, улица, фонарь.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(t, locale='en'):\n",
    "    from nltk.corpus import stopwords\n",
    "    from autocorrect import Speller\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    text = t.lower()\n",
    "    \n",
    "    if locale == 'ru':\n",
    "        stop_words = set(stopwords.words('russian'))\n",
    "        spell = Speller(lang='ru')\n",
    "    elif locale == 'en':\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        spell = Speller(lang='en')\n",
    "    else:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        spell = Speller(lang='en')\n",
    "        \n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "    filtered_sentence = [spell(w) for w in word_tokens if not spell(w) in stop_words and spell(w).strip() not in punctuation]\n",
    "\n",
    "    print(t)\n",
    "    print('to')\n",
    "    print(word_tokens)\n",
    "    print('to')\n",
    "    print(filtered_sentence)\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(en_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "vec = tokenizer.tokenize(en_text)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "|  The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n",
    "|  This is the method that is invoked by ``word_tokenize()``.  It assumes that the\n",
    "|  text has already been segmented into sentences, e.g. using ``sent_tokenize()``.\n",
    "|  \n",
    "|  This tokenizer performs the following steps:\n",
    "|  \n",
    "|  - split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n",
    "|  - treat most punctuation characters as separate tokens\n",
    "|  - split off commas and single quotes, when followed by whitespace\n",
    "|  - separate periods that appear at the end of line'''\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "vec2 = tokenizer.tokenize(text)\n",
    "print(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(en_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "|  Tokenize a text into a sequence of alphabetic and\n",
    "|  non-alphabetic characters, using the regexp ``\\w+|[^\\w\\s]+``.'''\n",
    "\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "vec3 = tokenizer.tokenize(text)\n",
    "print(vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec), len(vec2), len(vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vec)\n",
    "print(vec2)\n",
    "print(vec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ru_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "ru_vec = tokenizer.tokenize(ru_text)\n",
    "print(ru_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "ru_vec2 = tokenizer.tokenize(ru_text)\n",
    "print(ru_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(ru_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "ru_vec3 = tokenizer.tokenize(ru_text)\n",
    "print(ru_vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ru_vec), len(ru_vec2), len(ru_vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ru_vec,'\\n')\n",
    "print(ru_vec2, '\\n')\n",
    "print(ru_vec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(en_text)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "word_tokens = word_tokenize(ru_text)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens, '\\n')\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spell('caaar'))\n",
    "print(spell(u'mussage'))\n",
    "print(spell(u'survice'))\n",
    "print(spell(u'hte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='ru')\n",
    "print(spell('мышина'))\n",
    "print(spell(u'сбщение'))\n",
    "print(spell(u'проктика'))\n",
    "print(spell(u'каординация'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The United States of America (USA) or America, is a federal republic composed of 50 states\"\n",
    "print(text)\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stemming_sentence = [ps.stem(w) for w in custom_tokenizer(en_text)]\n",
    "print('to')\n",
    "print(en_stemming_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.stem('talked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_stemming_sentence = [ps.stem(w) for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_stemming_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "rs = RussianStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ru_stemming_sentence = [rs.stem(w) for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_stemming_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lemm_sentence = [lemmatizer.lemmatize(w) for w in custom_tokenizer(en_text)]\n",
    "print('to')\n",
    "print(en_lemm_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ru_lemm_sentence = [lemmatizer.lemmatize(w) for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_lemm_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var 1\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem() # lemmatize, analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pymystem](https://github.com/nlpub/pymystem3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.lemmatize('Стоять'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ru_lemm_sentence = [m.lemmatize(w)[0] for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_lemm_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var 2\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pymorphy](https://pymorphy2.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ru_lemm_sentence2 = [morph.parse(w)[0].normal_form for w in custom_tokenizer(ru_text)]\n",
    "print('to')\n",
    "print(ru_lemm_sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipline:\n",
    "- find unique words\n",
    "- enumerate it, map with dictionary\n",
    "- create zero vectors \n",
    "- fill the values inside loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'There used to be Stone Age\\nThere used to be Bronze Age\\nThere used to be Iron Age\\nThere was Age of Revolution\\nNow it is Digital Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = t.split('\\n') # \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(text):\n",
    "    wordfreq = {}\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "    return wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bow = bow(t)\n",
    "t_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF\n",
    "\n",
    "TF термина а = (Количество раз, когда термин а встретился в тексте / количество всех слов в тексте)\n",
    "\n",
    "IDF термина а = логарифм(Общее количество документов / Количество документов, в которых встречается термин а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "   'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    " ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.todense(), columns= vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ДЗ: проделать все то же самое с gensim (lemmatization)\n",
    "\n",
    "[Инструкция](https://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
